{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebooks covers the training code for Weight Watcher, a Food Classification, Calorie and Weight Estimation Deep Learning Bot\n\nFrom an image, it estimates the weight of the food and then estimates the calories of the dish.\n\nModel Strategy:\n- Segmentation for Pretraining, then classification + regression\nSegmentation:\n- Modified UNet + ASPP, to segment the coin and food(1 Yuan coin for scale, else weight estimation is impossible)\n- BackBone is then extracted(now that it has good features), and using the backbone, a Classification and Regression head is applied\n\nRegression: For Weight, calories can be directly looked up in table.\nclassification - Classify into 30 different foods.\n","metadata":{}},{"cell_type":"markdown","source":"# Import Dependencies","metadata":{}},{"cell_type":"markdown","source":"Potential Change: - Object Detection for Coin and Food?\n- 2x BBox Regression from image features(Use this if semantic seg fails)","metadata":{}},{"cell_type":"code","source":"%%capture\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim\nimport torchvision\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy\nimport numpy as np\nimport pandas as pd\nimport json\nimport cv2\n\nimport os\nimport math\nimport copy\nimport random\n\n!pip install efficientnet-pytorch\nfrom efficientnet_pytorch import EfficientNet\n\n!apt-get update\n!apt-get install libturbojpeg\n!pip install -U git+git://github.com/lilohuang/PyTurboJPEG.git\n\n\n!pip install PyTurboJPEG\nfrom turbojpeg import TurboJPEG\n\nimport pytorch_lightning as pl\nfrom fastai.vision.all import *\nfrom collections import Counter \n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as metrics\n!pip install xlrd\nimport xml.etree.ElementTree as ET\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport glob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reproducibility:\nimport os\nimport random\ndef seed_all():\n    seed = 42\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    # Slight Stochasticity Tradeoff for Quicker Comp.\n    torch.backends.cudnn.benchmark = False # True for faster\n    pl.seed_everything()\nseed_all()\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HELPER Fn's\ndef display_image_pt(image):\n    plt.imshow(image.cpu().transpose(0, 1).transpose(1, 2))\n    plt.show()\ndef display_image_np(image):\n    plt.imshow(image)\n    plt.show()\ndef display_image_mask(image, mask):\n    plt.imshow(image.cpu().transpose(0, 1).transpose(1, 2))\n    plt.imshow(mask, alpha = 0.5)\n    plt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseDataset(torch.utils.data.Dataset):\n    def __init__(self, image_base_path, train_df):\n        self.train_df = train_df\n        self.image_base_path = image_base_path\n        self.processed = self.process_dataframe()\n    def process_dataframe(self):\n        '''\n        Train_Df: Dataframe of 174 images.\n        Extracts the other 2978 images.\n        '''\n        processed = []\n        for row in self.train_df.iterrows():\n            row = row[1]\n            image_id = row.id\n            if 'mix' in image_id:\n                continue\n            class_val = row.type\n            volume = row['volume(mm^3)']\n            weight = row['weight(g)']\n            # Glob and Find all files with these statistics\n            files = glob.glob(f\"{self.image_base_path}{image_id}*\")\n            for file in files:\n                file_path = os.path.splitext(file)[0]\n                # Find First / and String away file path \n                string = ''\n                for idx in range(len(file_path) -1, -1, -1):\n                    if file_path[idx] == '/':\n                        string = file_path[idx + 1:]\n                        break\n                processed += [{string: {'volume': volume, 'weight': weight, 'class': class_val}}]\n        return processed","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_augmentations(IMAGE_SIZE):\n    # Heavy, Heavy Augmentation\n    train_transforms = A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.OneOf([\n            A.ColorJitter(p = 1, hue = 0.1, saturation = 0.1),\n            A.RandomGamma(p=1)\n        ], p=.5),\n        A.OneOf([\n            A.Blur(blur_limit=3, p=1),\n            A.MedianBlur(blur_limit=3, p=1)\n        ], p=.25),\n        A.OneOf([\n            A.GaussNoise(0.002, p=.5),\n            A.IAAAffine(p=.5),\n        ], p=.25),\n        A.OneOf([\n            A.ElasticTransform(alpha=120, sigma=120 * .05, alpha_affine=120 * .03, p=.5),\n            A.GridDistortion(p=.5),\n            A.OpticalDistortion(distort_limit=2, shift_limit=.5, p=1)                  \n        ], p=.25),\n        A.RandomRotate90(p=.5),\n        A.HorizontalFlip(p=.5),\n        A.Cutout(num_holes=10, \n                    max_h_size=int(.01 * IMAGE_SIZE), max_w_size=int(.01 * IMAGE_SIZE), \n                    p=.25),\n        A.ShiftScaleRotate(p=.5),\n        A.Normalize()\n    ])\n    test_transforms = A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.Normalize()\n    ])\n    return train_transforms, test_transforms\ndef load_excel(df_path):\n    df = pd.read_excel(df_path, sheet_name = None)\n    concat = None\n    for key in df:\n        if concat is None:\n            concat = df[key]\n        else:\n            concat = concat.append(df[key])\n    return concat\nclass Config:\n    annotation_base_path = '../input/fooddataset/dataset/ECUSTFD-resized--master/Annotations/'\n    image_base_path = '../input/fooddataset/dataset/ECUSTFD-resized--master/JPEGImages/'\n    df_path = '../input/fooddataset/dataset/ECUSTFD-resized--master/density.xls'\n    pretrained_model = '../input/pretrainedunet/best.pth' # Trained in prev version of the notebook.\n    df = load_excel(df_path)\n    files = BaseDataset(image_base_path, df).processed\n    # Split into Train and Val\n    train, val = train_test_split(files, train_size = 0.95, test_size = 0.05, random_state = 42)\n    # Weight Information -> Calories(Hard Coded)\n    weights2Cal ={\n        'apple': 0.52,\n        'banana': 0.89,\n        'bread': 3.15,\n        'bun':2.23,\n        'doughnut':4.34,\n        'egg': 1.43,\n        'fired_dough_twist': 24.16,\n        'grape': 0.69,\n        'lemon': 0.29,\n        'litchi': 0.66,\n        'mango': 0.60,\n        'mooncake': 18.83,\n        'orange': 0.63,\n        'peach': 0.57,\n        'pear': 0.39,\n        'plum': 0.46,\n        'qiwi': 0.61,\n        'sachima': 21.45,\n        'tomato': 0.27\n    }\n    \n    classes = sorted(weights2Cal.keys())\n    cls2idx = {}\n    idx2cls = {}\n    for idx in range(len(classes)):\n        cls2idx[classes[idx]] = idx\n        idx2cls[idx] = classes[idx]\n    num_classes = len(classes)\n    # Hyper Parameters\n    IMAGE_SIZE = 256\n    train_transforms, test_transforms = get_augmentations(IMAGE_SIZE)\n    to_tensor = ToTensorV2()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing:\n- Note: All images are smaller than 1024x1024, but there is only 2978 of them, so I expect heavy overfitting\n- Note also: Images are of variable size, thus I must create a segmentation mask first, then resize using Albumentations.","metadata":{}},{"cell_type":"code","source":" class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, dict_mapping, train = True):\n        self.dict_mapping = dict_mapping\n        self.TurboJPEG = TurboJPEG()\n        self.train = train\n    def __len__(self):\n        return len(self.dict_mapping)\n    def __getitem__(self, idx):\n        '''\n        Generates a Segmentation Mask for the Image \n        \n        0: Nothing there\n        1: Coin or Food\n        Trained with Lovask Loss\n        '''\n        row = self.dict_mapping[idx]\n        # Extract Data\n        ids = list(row.keys())[0]\n        image_path = Config.image_base_path + ids + '.JPG'\n        annot_path = Config.annotation_base_path + ids + '.xml'\n        # Load in Image\n        with open(image_path, 'rb') as file:\n            image = self.TurboJPEG.decode(file.read())\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # Load in XML\n        root = ET.parse(annot_path).getroot()\n        # Extract Major Headings\n        size = root[4]\n        food = root[6]\n        coin = root[7]\n        # Parse Further\n        width = int(size[0].text)\n        height = int(size[1].text)\n        \n        bnd_box_food = food[4]\n        x_min_food = int(bnd_box_food[0].text)\n        y_min_food = int(bnd_box_food[1].text)\n        x_max_food = int(bnd_box_food[2].text)\n        y_max_food = int(bnd_box_food[3].text)\n        \n        bnd_box_coin = coin[4]\n        x_min_coin = int(bnd_box_coin[0].text)\n        y_min_coin = int(bnd_box_coin[1].text)\n        x_max_coin = int(bnd_box_coin[2].text)\n        y_max_coin = int(bnd_box_coin[3].text)\n        \n        # Generate Segmentation Mask\n        seg_mask = np.zeros((height, width))\n        # Fill Coin\n        seg_mask[y_min_coin: y_max_coin, x_min_coin: x_max_coin] = 1\n        # Fill Food\n        seg_mask[y_min_food: y_max_food, x_min_food: x_max_food] = 1\n        # Augment Both Image and Mask\n        if self.train:\n            augmented = Config.train_transforms(image = image, mask = seg_mask)\n            image = Config.to_tensor(image = augmented['image'])['image']\n            seg_mask = augmented['mask']\n        else:\n            augmented = Config.test_transforms(image = image, mask = seg_mask)\n            image = Config.to_tensor(image = augmented['image'])['image']\n            seg_mask = augmented['mask']\n        return image, seg_mask \n            \nclass WeightWatcherDataset(torch.utils.data.Dataset):\n    def __init__(self, dict_mapping, train = True):\n        self.dict_mapping = dict_mapping\n        self.TurboJPEG = TurboJPEG()\n        self.train = train\n    def __len__(self):\n        return len(self.dict_mapping)\n    def __getitem__(self, idx):\n        ids = list(self.dict_mapping[idx])[0]\n        image_id = Config.image_base_path + ids + \".JPG\"\n        \n        GT = self.dict_mapping[idx][ids]\n        volume = GT['volume']\n        weight = GT['weight']\n        class_idx = Config.cls2idx[GT['class']]\n        \n        # Load in Image\n        with open(image_id, 'rb') as file:\n            image = self.TurboJPEG.decode(file.read())\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # Augment\n        if self.train:\n            image = Config.train_transforms(image = image)['image']\n        else:\n            image = Config.test_transforms(image = image)['image']\n        image = Config.to_tensor(image = image)['image']\n        return image, class_idx, volume, weight\n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataModule:\n    @classmethod\n    def get_seg(cls, train = True):\n        if train == True:\n            seg_Dataset = SegmentationDataset(Config.train, train = True)\n        else:\n            seg_Dataset = SegmentationDataset(Config.val, train = False)\n        return seg_Dataset\n    @classmethod\n    def get_cls(cls, train = True):\n        if train == True:\n            cls_Dataset = WeightWatcherDataset(Config.train, train = True)\n        else:\n            cls_Dataset = WeightWatcherDataset(Config.val, train = False)\n        return cls_Dataset","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Segmentation Pretraining\n- Design Custom UNet + ASPP Architecture to segment out the food and the coin, allowing it to learn key features about the image before doing anything about the \n- BackBone - EfficientNet + GhostNet Blocks + ASPP \n- Weak Decoder, Powerful Encoder, since we want most of the knowledge encoded in the encoder.","metadata":{}},{"cell_type":"markdown","source":"Handy CNN Blocks to Define(GhostNet, EffNet, ConvBlocks, etc.)","metadata":{}},{"cell_type":"code","source":"def initialize_weights(layer):\n    # Initialize weights using Kaiming init, better than Xavier.\n    for module in layer.modules():\n        if isinstance(module, (nn.Conv2d, nn.Conv1d)):\n            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n        elif isinstance(module, nn.BatchNorm2d):\n            module.weight.data.fill_(1)\n            module.bias.data.zero_()\nclass Mish(pl.LightningModule):\n    def __init__(self):\n        pass\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\ndef replace_mishes(model):\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.SiLU, nn.ReLU)):\n            setattr(model, name, Mish())\nclass Act(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.act_type = ModelConfig.act_type\n        if self.act_type == 'relu':\n            self.act = nn.ReLU(inplace = True)\n        elif self.act_type == 'mish':\n            self.act = Mish()\n        else:\n            self.act = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.act(x)\n\nclass ConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n    def forward(self, x):\n        mean = torch.squeeze(self.global_avg(x))\n        squeeze = self.act(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        return excite\nclass ECASqueezeExcite(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.kernel_size = 5\n        self.padding = 2\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv = nn.Conv1d(1, 1, kernel_size = self.kernel_size, padding = self.padding, bias = False)\n        initialize_weights(self)\n    def forward(self, x):\n        mean = torch.squeeze(self.global_avg(x), dim = -1).transpose(-1, -2) # (B, 1, C)\n        conv = torch.sigmoid(self.conv(mean)).transpose(-1, -2).unsqueeze(-1) # (B, C, 1, 1)\n        return conv * x\nclass SCSqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.kernel_size = 3\n        self.padding = 1\n        \n        self.Conv_Squeeze = nn.Conv2d(self.in_features, 1, kernel_size = self.kernel_size, padding = self.padding, bias = False)\n        initialize_weights(self)\n    def forward(self, x):\n        mean = torch.squeeze(self.global_avg(x))\n        squeeze = self.act(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        \n        conv_excite = torch.sigmoid(self.Conv_Squeeze(x)) * x\n        excited = (excite + conv_excite) / 2\n        return excited\nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.attention_type = ModelConfig.attention_type\n        assert self.attention_type in ['eca', 'se', 'scse', 'none']\n        if self.attention_type == 'eca':\n            self.layer = ECASqueezeExcite()\n        elif self.attention_type == 'se':\n            self.layer = SqueezeExcite(self.in_features, self.inner_features)\n        elif self.attention_type == 'scse':\n            self.layer = SCSqueezeExcite(self.in_features, self.inner_features)\n        else:\n            self.layer = nn.Identity()\n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        excited = self.layer(x)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excited + (1 - gamma) * x\n        return excited\nclass SwinTransformerAttention(pl.LightningModule):\n    def __init__(self, length, in_features, inner_features, num_heads):\n        super().__init__()\n        self.length = length \n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n    \n        self.Keys = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Queries = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Values = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Linear = nn.Linear(self.inner_features * self.num_heads, self.in_features)\n        \n        \n        self.pos_enc = nn.Parameter(nn.init.xavier_uniform_(torch.zeros((self.num_heads, self.length, self.length), device = self.device)))\n    def forward(self, x):\n        B, L, _ = x.shape\n        assert L == self.length\n        K = self.Keys(x)\n        V = self.Values(x)\n        Q = self.Queries(x) # (B, L, Heads * Inner_features)\n        \n        K = K.view(B, L, self.num_heads, self.inner_features)\n        V = V.view(B, L, self.num_heads, self.inner_features)\n        Q = Q.view(B, L, self.num_heads, self.inner_features)\n        \n        K = K.transpose(1, 2).view(-1, L, self.inner_features)\n        V = V.transpose(1, 2).view(-1, L, self.inner_features)\n        Q = Q.transpose(1, 2).view(-1, L, self.inner_features) # (BH, L, inner_features)\n        \n        pos_enc = torch.repeat_interleave(self.pos_enc, B, dim = 0)\n        att_mat = F.softmax(Q @ K.transpose(1, 2) / math.sqrt(self.inner_features) + pos_enc, dim = -1) # (BH, L, L)\n        att_scores = att_mat @ V # (BH, L, I)\n        \n        att_scores = att_scores.view(B, self.num_heads, L, self.inner_features)\n        att_scores = att_scores.transpose(1, 2).view(B, L, -1)\n        return self.Linear(att_scores)\nclass SwinTransformerEncoder(pl.LightningModule):\n    def window(self, x):\n        # X: Tensor(B, C, H, W) \n        B, C, H, W = x.shape\n        windowed = x.view(B, C, H // self.window_size, self.window_size, W // self.window_size, self.window_size)\n        windowed = x.permute(0, 2, 4, 3, 5, 1)\n        windowed = torch.view(-1, self.window_size * self.window_size, C)\n        return windowed\n    def unwindow(self, x):\n        B, _, C = x.shape\n        B = B // (self.H // self.window_size) // (self.W // self.window_size)\n        \n        unwindow = x.view(B, self.H // self.window_size, self.W // self.window_size, self.window_size, self.window_size, C)\n        unwindow = unwindow.permute(0, 5, 1, 3, 2, 4) \n        unwindow = unwindow.view(B, C, self.H, self.W)\n        return unwindow\n        \n    def shift(self, x):\n        return torch.roll(x, (-(self.window_size // 2), -(self.window_size // 2)))\n    def unshift(self, x):\n        return torch.roll(x, (self.window_size // 2, self.window_size // 2))\n    def __init__(self, H, W, in_features, inner_features, out_features, num_heads, window_size = 4):\n        super().__init__()\n        self.H = H\n        self.W = W\n        self.length = self.H * self.W\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.num_heads = num_heads\n        self.window_size = window_size\n        \n        self.pos_enc = nn.Parameter(nn.init.xavier_uniform_(torch.zeros((1, self.in_features, self.H, self.W), device = self.device)))\n        self.norm1 = nn.LayerNorm((self.in_features, self.H, self.W))\n        self.att1 = SwinTransformerAttention(self.window_size ** 2, self.in_features, self.inner_features, self.num_heads)\n        self.norm2 = nn.LayerNorm((self.length, self.in_features))\n        self.linear2 = nn.Linear(self.in_features, self.in_features)\n        \n        self.norm3 = nn.LayerNorm((self.in_features, self.H, self.W))\n        self.att3 = SwinTransformerAttention(self.window_size ** 2, self.in_features, self.inner_features, self.num_heads)\n        self.norm4 = nn.LayerNorm((self.length, self.in_features))\n        self.linear4 = nn.Linear(self.in_features, self.out_features)\n    def forward(self, x):\n        # X: Tensor(B, C, H, W)\n        x = x + self.pos_enc # (B, C, H, W)\n        norm1 = self.norm1(x) # (B, C, H, W)\n        windowed = self.window(norm1) # (-1, window_size ** 2, C)\n        att1 = self.att1(windowed)\n        # Unwindow\n        unwindowed = self.unwindow(att1) + x# (B, C, H, W)\n        unwindowed = unwindowed.view(B, C, self.W * self.H).transpose(1, 2) # (B, HW, C)\n        norm2 = self.norm2(unwindowed) # (B, HW, C)\n        linear2 = self.linear2(norm2) + unwindowed\n    \n        linear2 = linear2.transpose(1, 2).view(B, C, self.H, self.W)\n        norm3 = self.norm3(linear2) # (B, C, H, W)\n        windowed = self.window(norm3) \n        windowed = self.shift(windowed)\n        \n        att3 = self.att3(windowed)\n        att3 = self.unshift(att3)\n        \n        unwindowed = self.unwindow(att3) + linear2 # (b, C, H, W)\n        \n        unwindowed = unwindowed.view(B, C, self.H * self.W).transpose(1, 2) # (B, L, C)\n        norm4 = self.norm4(unwindowed) \n        linear4 = self.linear4(norm4) + unwindowed# (B, L, C) \n    \n        output = linear4.transpose(1, 2).view(B, C, self.H, self.W)\n        return output\n        \n        \n        \nclass SplitAttention(pl.LightningModule):\n    # split attention like in ResNest.\n    def __init__(self, in_features, inner_features, cardinality):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.cardinality = cardinality\n        assert self.in_features % self.cardinality == 0\n        \n        self.AvgPool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv1 = ConvBlock(self.in_features, self.inner_features * self.cardinality, 1, 0, 1, 1)\n        self.conv2 = nn.Conv2d(self.inner_features * self.cardinality, self.in_features * self.cardinality, kernel_size = 1, groups = self.cardinality, bias = False)\n        initialize_weights(self.conv2)\n        \n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n        \n    def forward(self, x):\n        # X: Tensor(B, C, H, W, Cardinality)\n        B, C, H, W, Cardinality = x.shape\n        summed = torch.sum(x, dim = -1)\n        pooled = self.AvgPool(summed) # (B, C * Cardinality, 1, 1)\n        \n        conv1 = self.conv1(pooled)\n        conv2 = self.conv2(conv1) # (B, inner_features * cardinality, 1, 1)  \n        conv2 = conv2.view(B, self.in_features, self.cardinality)# (B, inner_features, cardinality, 1, 1)\n        conv2 = F.softmax(conv2.unsqueeze(2).unsqueeze(2), dim = -1) # (B, inner_features, 1, 1, cardinality)\n        \n        excited = x * conv2\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excited + (1 - gamma) * x\n        return excited\nclass GhostConv(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.padding = padding\n        \n        self.inner_features = self.out_features // 2\n        \n        self.squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.cheap = ConvBlock(self.inner_features, self.inner_features, self.kernel_size, self.padding, self.inner_features, 1)\n    def forward(self, x):\n        squeeze = self.squeeze(x)\n        cheap = self.cheap(squeeze) # (B, C, H, W)\n        \n        return torch.cat([squeeze, cheap], dim = 1) \n\nclass AstrousConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\n        \nclass BAM(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        self.bam_dilate = ModelConfig.bam_dilate\n        \n        self.ConvSqueeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DA = AstrousConvBlock(self.inner_features, self.inner_features, 3, self.bam_dilate, self.inner_features, 1, self.bam_dilate)\n        self.ConvExcite = nn.Conv2d(self.inner_features, 1, kernel_size = 1, bias = False)\n        initialize_weights(self.ConvExcite)\n        \n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        pooled = torch.squeeze(self.global_avg(x))\n        squeeze = self.Act(self.Squeeze(pooled))\n        excite = torch.sigmoid(self.Excite(squeeze).unsqueeze(-1).unsqueeze(-1)) * x\n        \n        squeeze = self.ConvSqueeze(x)\n        DA = self.DA(squeeze)\n        convExcite = self.ConvExcite(DA) * x\n        \n        excited = (convExcite + excite) / 2\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excited + (1 - gamma) * x\n        return excited\n    \n# -----------------BottleNeck Blocks -------------------#\nclass GhostBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.ghost1 = GhostConv(self.in_features, self.inner_features, 1, 0)\n        self.att = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.ghost2 = GhostConv(self.inner_features, self.in_features, 3, 1)\n        \n        self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        ghost1 = self.ghost1(x)\n        att1 = self.att(ghost1)\n        ghost2 = self.ghost2(att1)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * ghost2 + (1 - gamma) * x\nclass GhostDownSampler(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.reduction = ModelConfig.reduction\n        \n        self.AvgPool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.ghostPool = GhostConv(self.in_features, self.out_features, 1, 0)\n        \n        self.Ghost1 = GhostConv(self.in_features, self.inner_features, 1, 0)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.att = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Ghost2 = GhostConv(self.inner_features, self.out_features, 1, 0)\n        \n        self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        pooled = self.AvgPool(x)\n        ghostPool = self.ghostPoool(pooled)\n        \n        ghost1 = self.Ghost1(x)\n        dw = self.DW(ghost1)\n        att = self.att(dw)\n        ghost2 = self.Ghost2(att)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * ghostPool + (1 - gamma) * ghost2\nclass ResNextBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, cardinality, resnest = False):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.cardinality = cardinality\n        self.resnest = resnest\n        self.reduction = ModelConfig.reduction\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features * self.cardinality, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features * self.cardinality, self.inner_features * self.cardinality, 3, 1, self.cardinality, 1)\n        if self.resnest:\n            #  Split Attention\n            self.att = SplitAttention(self.inner_features, self.inner_features // self.reduction, self.cardinality)\n        else:\n            self.att = Attention(self.inner_features * self.cardinality, self.inner_features * self.cardinality // self.reduction)\n        self.Expand = ConvBlock(self.inner_features * self.cardinality, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        if self.resnest:\n            B, C, H, W = process.shape\n            process = process.view(B, C // self.cardinality, self.cardinality, H, W)\n            process = process.transpose(2, 3).transpose(3, 4)\n            \n            att = self.att(process)\n            att = att.transpose(3, 4).transpose(2, 3).view(B, -1, H, W)\n        else:\n            att = self.att(process)\n        expand = self.Expand(att)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * expand + (1 - gamma) * x\nclass ResNextDownSampler(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride, cardinality, resnest = False):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.cardinality = cardinality\n        self.resnest = resnest \n        self.reduction = ModelConfig.reduction\n        \n        self.avgPool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.ConvPool = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features * self.cardinality, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.cardinality * self.inner_features, self.inner_features * self.cardinality, 3, 1, self.cardinality, 1)\n        if self.resnest:\n            self.att = SplitAttention(self.inner_features, self.inner_features // self.reduction, self.cardinality)\n        else:\n            self.att = Attention(self.inner_features * self.cardinality, self.inner_features * self.cardinality // self.reduction)\n        self.Expand = ConvBlock(self.inner_features * self.cardinality, self.in_features, 1, 0, 1, 1)\n    \n        self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        pooled = self.avgPool(x)\n        convPool = self.ConvPool(pooled)\n        \n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        if self.resnest:\n            B, C, H, W = process.shape\n            process = process.view(B, C // self.cardinality, self.cardinality, H, W)\n            process = process.transpose(2, 3).transpose(3, 4)\n            att = self.att(process)\n            att = att.transpose(3, 4).transpose(2, 3).view(B, -1, H, W) \n        else:\n            att = self.att(process)\n        expand = self.Expand(att)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * expand + (1 - gamma) * convPool\nclass InverseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.Attention = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        att = self.Attention(dw)\n        squeeze = self.Squeeze(att)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * squeeze + (1 - gamma) * x\nclass InverseDownSampler(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride \n        self.reduction = ModelConfig.reduction\n    \n        self.AvgPool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.ConvPool = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1)\n    \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.Attention = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Squeeze = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.tensor(-10.0, device = self.device))\n    def forward(self, x):\n        pooled = self.AvgPool(x)\n        convPool = self.ConvPool(pooled)\n        \n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        att = self.Attention(dw)\n        squeeze = self.Squeeze(att)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * squeeze + (1 - gamma) * convPool\nclass ChooseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.bottleneck_type = ModelConfig.bottleneck_type\n        \n        if self.bottleneck_type == 'ghost':\n            self.layer = GhostBottleNeck(self.in_features, self.inner_features)\n        elif self.bottleneck_type == 'inverse':\n            self.layer = InverseBottleNeck(self.in_features, self.inner_features)\n        else:\n            self.cardinality = ModelConfig.groups\n            self.resnest = ModelConfig.resnest\n            self.layer = ResNextBottleNeck(self.in_features, self.inner_features, self.cardinality, resnest = self.resnest)\n    def forward(self, x):\n        return self.layer(x)\n        \nclass ChooseDownsampler(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.bottleneck_type = ModelConfig.bottleneck_type\n    \n        if self.bottleneck_type == 'ghost':\n            self.layer = GhostDownSampler(self.in_features, self.inner_features, self.out_features, self.stride)\n        elif self.bottleneck_type == 'inverse':\n            self.layer = InverseDownSampler(self.in_features, self.inner_features, self.out_features, self.stride)\n        else:\n            self.cardinality = ModelConfig.groups\n            self.resnest = ModelConfig.resnest\n            self.layer = ResNextDownSampler(self.in_features, self.inner_features, self.out_features, self.stride, self.cardinality, resnest = self.resnest)\n    def forward(self, x):\n        return self.layer(x)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Segmentation Specific Blocks(ASPP, DC-UNet, swin Transformer Block)","metadata":{}},{"cell_type":"code","source":"class ASPP(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        '''\n        ASPP Module:\n        - 1x1 Conv\n        - 3x3 Conv, Astrous 2\n        - 3x3 Conv, Astrous 3\n        - 3x3,Conv, Astrous 5\n        - 3x3 Pool\n        '''\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride \n        \n        self.conv1 = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, self.stride)\n        self.conv2 = AstrousConvBlock(self.in_features, self.inner_features, 3, 2, 1, self.stride, 2)\n        self.conv3 = AstrousConvBlock(self.in_features, self.inner_features, 3, 3, 1, self.stride, 3)\n        self.conv4 = AstrousConvBlock(self.in_features, self.inner_features, 3, 5, 1, self.stride, 5)\n        self.conv5 = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        \n        self.ConvProj = ConvBlock(self.inner_features * 4 + self.in_features, self.out_features, 1, 0, 1, 1)\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(x)\n        conv3 = self.conv3(x)\n        conv4 = self.conv4(x)\n        conv5 = self.conv5(x)\n        \n        concat = torch.cat([conv1, conv2, conv3, conv4, conv5], dim = 1)\n        return self.ConvProj(concat)\n        \nclass DualChannelBlock(pl.LightningModule):\n    # DC-Unet Block, However only 1 channels to reduce model parameters\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        \n        self.Conv1 = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 1)\n        self.Conv2 = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        \n        self.Conv3 = ConvBlock(self.inner_features * 2 + self.in_features, self.out_features, 3, 1, 1, self.stride)\n    def forward(self, x):\n        conv1 = self.Conv1(x)\n        conv2 = self.Conv2(conv1)\n        cat = torch.cat([x, conv1, conv2], dim = 1) \n        conv3 = self.Conv3(cat)\n        return conv3\nclass FPN(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size):\n        super().__init__()\n        self.in_features = in_features\n        self.num_features = len(self.in_features)\n        self.inner_features = inner_features\n        self.out_features = out_features\n        \n        self.convBlocks = nn.ModuleList([\n            ConvBlock(self.in_features[idx], self.inner_features) for idx in range(self.num_features)\n        ])\n        self.Proj = ConvBlock(self.inner_features * self.num_features, self.out_features)\n    def forward(self, features):\n        concat_features = []\n        for idx in range(len(features)):\n            conv_feature = self.convBlocks[idx](features[idx])\n            conv_feature = F.interpolate(conv_feature, size = (self.out_size, self.out_size), mode = 'bilinear')\n            concat_features += [conv_feature]\n        concat_features = torch.cat(concat_features, dim = 1) \n        return self.Proj(concat_features)\n        \nclass GatedAttentionBlock(pl.LightningModule):\n    # Gated Attention Block, like in Attention-UNet\n    def __init__(self, left_features, down_features, inner_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.inner_features = inner_features\n        \n        self.GlobalPool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze_Left = nn.Linear(self.left_features, self.inner_features)\n        self.Squeeze_Down = nn.Linear(self.down_features, self.inner_features)\n        \n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.down_features)\n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.tensor(-10, device = self.device))\n    def forward(self, left_features, down_features):\n        left_features = self.GlobalPool(left_features)\n        down_features = self.GlobalPool(down_features)\n         \n        squeeze_left = self.Squeeze_Left(left_features)\n        squeeze_down = self.Squeeze_Down(down_features)\n        \n        squeezed = self.Act((squeeze_left + squeeze_down) / 2)\n        excited = torch.sigmoid(self.Excite(squeezed)).unsqueeze(-1).unsqueeze(-1)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            excited = gamma * excited + (1 - gamma) * x\n        return excited\nclass UNetBlock(pl.LightningModule):\n    def __init__(self, left_features, down_features, out_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        self.reduction = ModelConfig.reduction\n        \n        self.decoder_attention = ModelConfig.decoder_attention\n        if self.decoder_attention:\n            self.att = GatedAttentionBlock(self.left_features, self.down_features, self.down_features // self.reduction)\n        self.conv1 = ConvBlock(self.down_features + self.left_features, self.out_features, 3, 1, 1, 1)\n        self.conv2 = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1)\n        if self.decoder_attention:\n            self.att = Attention(self.out_features, self.out_features // self.reduction)\n        else:\n            self.att = nn.Identity()\n    def forward(self, left_features, down_features):\n        if left_features is None:\n            down_features = F.interpolate(down_features, scale_factor = 2, mode = 'nearest')\n            concat = down_features\n        else:\n            down_features = F.interpolate(down_features, scale_factor = 2, mode = 'nearest')\n            if self.decoder_attention:\n                down_features = self.att(left_features, down_features)\n            concat = torch.cat([down_features, left_features], dim = 1) \n        conv1 = self.conv1(concat)\n        conv2 = self.conv2(conv1)\n        return self.att(conv2)    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segmentation model","metadata":{}},{"cell_type":"code","source":"class EncoderAlpha(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def freeze_cls(self):\n        self.freeze([self.conv1, self.bn1, self.block0, self.block1, self.block2, self.block3, self.block4, self.block5])\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'efficientnet-b0'\n        self.model = EfficientNet.from_pretrained(self.model_name)\n        \n        # Extract Layers\n        self.conv1 = self.model._conv_stem # 32, 128\n        self.bn1 = self.model._bn0\n        self.act1 = self.model._swish\n        \n        self.block0 = self.model._blocks[0] # 16, 128\n        self.block1 = nn.Sequential(*self.model._blocks[1:3]) # 24, 64 \n        self.block2 = nn.Sequential(*self.model._blocks[3: 5]) # 40, 32\n        self.block3 = nn.Sequential(*self.model._blocks[5: 8]) # 80, 16\n        self.block4 = nn.Sequential(*self.model._blocks[8: 11]) # 112, 16\n        self.block5 = nn.Sequential(*self.model._blocks[11: 15]) # 192, 8\n        self.block6 = self.model._blocks[15] # 320, 8\n        \n        # Freeze Initial Layers\n        self.freeze([self.conv1, self.bn1, self.block0, self.block1])\n    \n        self.reduction = ModelConfig.bottleneck_reduction\n        \n        self.Dropout6 = nn.Dropout2d(ModelConfig.drop_prob)\n        self.Attention6 = BAM(320, 320 // self.reduction)\n        \n        self.block7 = nn.Sequential(*[\n            ChooseBottleNeck(320, 320 // self.reduction) for i in range(2)\n        ] + [\n            DualChannelBlock(320, 320 // self.reduction, 320, 1),\n            ASPP(320, 320 // self.reduction, 512, 2)\n        ])\n        \n        self.Dropout7 = nn.Dropout2d(ModelConfig.drop_prob)\n        self.Attention7 = BAM(512, 512 // self.reduction)\n        \n        # large encoder, small decoder(Since it's being stripped away)\n    def forward_cls(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x)))\n        block0 = self.block0(features0)\n        block1 = self.block1(block0)\n        block2 = self.block2(block1)\n        block3 = self.block3(block2)\n        block4 = self.block4(block3)\n        block5 = self.block5(block4)\n        block6 = self.block6(block5)\n        \n        block6 = self.Attention6(self.Dropout6(block6))\n        block7 = self.block7(block6)\n        block7 = self.Attention7(self.Dropout7(block7))\n        return block7\n    def forward(self, x):\n        features0= self.bn1(self.act1(self.conv1(x)))\n        block0 = self.block0(features0) # 16\n        block1 = self.block1(block0) # 24\n        block2 = self.block2(block1) # 40\n        block3 = self.block3(block2) # 80\n        block4 = self.block4(block3) # 112\n        block5 = self.block5(block4) # 192\n        block6 = self.block6(block5) # 320\n        \n        block6 = self.Attention6(self.Dropout6(block6))\n        block7 = self.block7(block6)\n        block7 = self.Attention7(self.Dropout7(block7)) # 512\n        \n        \n        features = [block0, block1, block2, block4, block6, block7]\n        return features\nclass DecoderAlpha(pl.LightningModule):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n        self.enc_dims = [320, 112, 40, 24, 16, 0]\n        self.dec_dims = [512, 256, 128, 64, 32, 16, 16]\n        \n        self.decoder_blocks = nn.ModuleList([\n            UNetBlock(self.enc_dims[idx], self.dec_dims[idx], self.dec_dims[idx + 1]) for idx in range(len(self.enc_dims))\n        ])\n        \n        self.use_FPN = ModelConfig.use_FPN\n        if self.use_FPN:\n            self.FPN = FPN(self.dec_dims[1:-1], self.dec_dims[-1], self.dec_dims[-1], Config.IMAGE_SIZE // 2)\n        self.segmentation_head = nn.Conv2d(self.dec_dims[-1], ModelConfig.num_classes, kernel_size = 3, padding = 1)\n        initialize_weights(self.segmentation_head)\n    def forward(self, features):\n        l0, l1, l2, l3, l4, l5 = tuple(features)\n        \n        d4 = self.decoder_blocks[0](l4, l5) # 256\n        d3 = self.decoder_blocks[1](l3, d4) # 128\n        d2 = self.decoder_blocks[2](l2, d3) # 64,\n        d1 = self.decoder_blocks[3](l1, d2) # 32\n        d0 = self.decoder_blocks[4](l0, d1) # 16\n        \n        if self.use_FPN:\n            d0 = self.FPN([d4, d3, d2, d1, d0])\n        \n        final = self.decoder_blocks[5](None, d0)\n        return self.segmentation_head(final)\n        \nclass SegmentationModelAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_classes = ModelConfig.num_classes\n        self.encoder = EncoderAlpha()\n        self.decoder = DecoderAlpha(self.num_classes)\n    def forward(self, x):\n        features = self.encoder(x)\n        pred = self.decoder(features)\n        return pred","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Segmentation Model","metadata":{}},{"cell_type":"code","source":"class ModelConfig:\n    num_classes = 2\n    drop_prob = 0.1\n    act_type = 'relu'\n    gate_attention = True\n    attention_type = 'se'\n    reduction = 16\n    \n    bam_dilate = 3\n    groups = 16\n    resnest = True\n    \n    bottleneck_type = 'resnext'\n    bottleneck_reduction = 4\n    \n    decoder_attention = False\n    use_FPN = False","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics and LossFn","metadata":{}},{"cell_type":"code","source":"CRITERION = nn.CrossEntropyLoss()\ndef dice_loss(y_pred, y_true):\n    y_pred = F.softmax(y_pred, dim = 1)\n    y_ones = y_pred[:, 1, :, :]\n    \n    eps = 1e-8\n    inter = torch.sum(y_ones * y_true)\n    union = torch.sum(y_ones + y_true)\n    \n    dice = (2 * inter + eps) / (union + eps)\n    loss = 1 - dice\n    return torch.log((torch.exp(loss) + torch.exp(-loss)) / 2)\ndef ce_loss(y_pred, y_true):\n    loss = CRITERION(y_pred, y_true.to(torch.long))\n    return loss\ndef loss_fn(y_pred, y_true):\n    ce = ce_loss(y_pred, y_true)\n    dice = dice_loss(y_pred, y_true)\n    return ce + dice\nclass Loss(Metric):\n    def __init__(self):\n        super().__init__()\n        self.loss = 0\n        self.count = 0\n    def reset(self):\n        self.loss = 0\n        self.count = 0\n    def accumulate(self, y_pred, y_true):\n        loss = loss_fn(y_pred, y_true)\n        self.loss += loss.item()\n        self.count += 1\n        return loss\n    @property\n    def value(self):\n        if self.count != 0:\n            return self.loss / self.count \n        return 0\nclass DiceMetric(Metric):\n    # Dice Soft Metric\n    def __init__(self):\n        super().__init__()\n        self.inter = 0\n        self.union = 0\n    def reset(self):\n        self.inter = 0\n        self.union = 0\n    def inter_union(self, y_pred, y_true):\n        y_ones = y_pred[:, 1, :, :]\n        self.inter += torch.sum(y_ones * y_true)\n        self.union += torch.sum(y_ones + y_true)\n        \n    def accumulate(self, y_pred, y_true):\n        y_pred = F.softmax(y_pred, dim = 1)\n        self.inter_union(y_pred, y_true)\n    @property\n    def value(self):\n        eps = 1e-8\n        dice = (2 * self.inter + eps) / (self.union + eps)\n        return round(dice.item(), 3)\n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainConfig:\n    lr = 1e-3\n    num_epochs = 10000 # Dummy Number that is never reached\n    batch_size = 64\n    sample_train = DataModule.get_seg()\n    \n    num_steps_per_epoch = len(sample_train) // batch_size\n    weight_decay = 1e-1\n    num_steps = 5\n    eta_min = 1e-7\n    max_lr = 1e-2\n    num_workers = 4\n    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainerSeg(pl.LightningModule):\n    # I don't have much time in this hackathon, so I'm only going to train for 20 or 30 minutes(30 epochs) could probably boost performance with more pretraining\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n        \n        self.TrainLoss = Loss()\n        self.ValLoss = Loss()\n        self.ValDice = DiceMetric()\n        \n        self.best = {'loss': float('inf'), 'dice': 0.0}\n        self.EPOCHS = 0\n    def configure_model(self):\n        model = SegmentationModelAlpha()\n        return model \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr = TrainConfig.lr, weight_decay = TrainConfig.weight_decay)\n        self.lr_decay_cycle = optim.lr_scheduler.OneCycleLR(optimizer, TrainConfig.max_lr, total_steps = TrainConfig.num_steps_per_epoch * TrainConfig.num_epochs, epochs = TrainConfig.num_epochs, steps_per_epoch = TrainConfig.num_steps_per_epoch)\n        return optimizer\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n    \n        pred = self.model(x)\n        loss = self.TrainLoss.accumulate(pred, y)\n        self.lr_decay_cycle.step()\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch \n    \n        pred = self.model(x)\n        loss = self.ValLoss.accumulate(pred, y)\n        self.ValDice.accumulate(pred, y)\n    def reset_states(self):\n        self.TrainLoss.reset()\n        self.ValLoss.reset()\n        self.ValDice.reset()\n        self.EPOCHS += 1\n    def save_states(self):\n        TrainLoss = self.TrainLoss.value\n        ValLoss = self.ValLoss.value\n        ValDice = self.ValDice.value\n        self.log('val_dice', ValDice)\n        if ValLoss < self.best['loss']:\n            self.best['loss'] = ValLoss\n            torch.save(self.model.encoder.state_dict(), './best.pth')\n        if ValDice > self.best['dice']:\n            self.best['dice'] = ValDice\n            torch.save(self.model.encoder.state_dict(), './dice.pth')\n        print(f\"E: {self.EPOCHS}, TL: {TrainLoss}, VL: {ValLoss} VD: {ValDice}, BL: {self.best['loss']}, BD: {self.best['dice']}\")\n        \n    def validation_epoch_end(self, logs):\n        self.save_states()\n        self.reset_states()\n        \n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_seg():\n    train = DataModule.get_seg()\n    val = DataModule.get_seg(train = False)\n    \n    model = TrainerSeg()\n    dls = DataLoaders.from_dsets(train, val, batch_size = TrainConfig.batch_size, num_workers = TrainConfig.num_workers, shuffle = True, pin_memory = True)\n    if torch.cuda.is_available(): dls.cuda(), model.cuda()\n    cbs = [pl.callbacks.EarlyStopping(monitor = 'val_dice', patience = 10, mode = 'max'\n    \n    )]\n    trainer = pl.Trainer(check_val_every_n_epoch = 1, callbacks = cbs, checkpoint_callback = False, logger = None, gpus = 1, max_epochs = TrainConfig.num_epochs, num_sanity_val_steps=0)\n    trainer.fit(model, dls[0], dls[1])\n    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_seg()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Classification and Regression Heads.\n- Cross Stitched Classification and Regression Heads","metadata":{}},{"cell_type":"code","source":"class FeaturesAlpha(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def __init__(self):\n        super().__init__()\n        self.model = EncoderAlpha()\n        self.model.load_state_dict(torch.load(Config.pretrained_model, map_location = self.device))\n        # Freeze Entire Encoder\n        self.model.freeze_cls()\n        # One Last Layer, seperated by Task.\n        self.in_dim = 512\n        self.out_dim = 1024\n        self.reduction = ModelConfig.reduction\n        self.cls_att8 = nn.Identity()#BAM(self.in_dim, self.in_dim // self.reduction)\n        self.reg_att8 = nn.Identity()#BAM(self.in_dim, self.in_dim // self.reduction)\n        self.vol_att8 = nn.Identity()#BAM(self.in_dim, self.in_dim // self.reduction)\n        self.drop_att8 = nn.Identity()#nn.Dropout2d(0.1)\n        \n        self.proj_cls = ConvBlock(self.in_dim, self.out_dim, 1, 0, 1, 1)\n        self.proj_reg = ConvBlock(self.in_dim, self.out_dim, 1, 0, 1, 1)\n        self.proj_vol = ConvBlock(self.in_dim, self.out_dim, 1, 0, 1, 1)\n        \n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        features = self.model.forward_cls(x)\n    \n        cls_att8 = self.cls_att8(self.drop_att8(features))\n        reg_att8 = self.reg_att8(self.drop_att8(features))\n        vol_att8 = self.vol_att8(self.drop_att8(features))\n        \n        proj_cls = self.proj_cls(cls_att8)\n        proj_reg = self.proj_reg(reg_att8)\n        proj_vol = self.proj_vol(vol_att8)\n        \n        pooled_cls = torch.squeeze(self.global_avg(proj_cls))\n        pooled_reg = torch.squeeze(self.global_avg(proj_reg))\n        pooled_vol = torch.squeeze(self.global_avg(proj_vol))\n        \n        return pooled_cls, pooled_reg, pooled_vol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CrossStitchUnit(pl.LightningModule):\n    # Merges Features from Both Branches to better MultiTask Learn\n    def __init__(self):\n        super().__init__()\n        self.alpha1_1 = nn.Parameter(torch.tensor(1., device = self.device))\n        self.alpha1_2 = nn.Parameter(torch.tensor(0., device = self.device))\n        self.alpha1_3 = nn.Parameter(torch.tensor(0., device = self.device))\n        \n        self.alpha2_1 = nn.Parameter(torch.tensor(0., device = self.device))\n        self.alpha2_2 = nn.Parameter(torch.tensor(1., device = self.device))\n        self.alpha2_3 = nn.Parameter(torch.tensor(0., device = self.device))\n        \n        self.alpha3_1 = nn.Parameter(torch.tensor(0., device = self.device))\n        self.alpha3_2 = nn.Parameter(torch.tensor(0., device = self.device))\n        self.alpha3_3 = nn.Parameter(torch.tensor(1., device = self.device))\n        \n    def forward(self, CLS, REG, VOL):\n        alpha1_1 = torch.sigmoid(self.alpha1_1)\n        alpha1_2 = torch.sigmoid(self.alpha1_2)\n        alpha1_3 = torch.sigmoid(self.alpha1_3)\n        \n        alpha2_1 = torch.sigmoid(self.alpha2_1)\n        alpha2_2 = torch.sigmoid(self.alpha2_2)\n        alpha2_3 = torch.sigmoid(self.alpha2_3)\n        \n        alpha3_1 = torch.sigmoid(self.alpha3_1)\n        alpha3_2 = torch.sigmoid(self.alpha3_2)\n        alpha3_3 = torch.sigmoid(self.alpha3_3)\n        \n        new_cls = alpha1_1 * CLS + alpha1_2 * REG + alpha1_3 * VOL\n        new_reg = alpha2_1 * CLS + alpha2_2 * REG + alpha2_3 * VOL\n        new_vol = alpha3_1 * CLS + alpha3_2 * REG + alpha3_3 * VOL\n        \n        return new_cls, new_reg, new_vol\nclass LinBNReLU(pl.LightningModule):\n    def __init__(self, in_features, out_features, drop_prob = 0.5):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.drop_prob = drop_prob\n        self.layer = nn.Sequential(*[\n            nn.Linear(self.in_features, self.out_features),\n            nn.ReLU(inplace = True),\n            nn.BatchNorm1d(self.out_features),\n            nn.Dropout(self.drop_prob)\n        ])\n    def forward(self, x):\n        return self.layer(x)\nclass BaseLineHeadAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.in_dim = 1024\n        self.num_classes = Config.num_classes\n        self.fc_cls = nn.Linear(self.in_dim, self.num_classes)\n        self.fc_reg = nn.Linear(self.in_dim, 1)\n        self.fc_vol = nn.Linear(self.in_dim, 1)\n    def forward(self, CLS, REG, VOL):\n        return self.fc_cls(CLS), torch.squeeze(self.fc_reg(REG)), torch.squeeze(self.fc_vol(VOL))\nclass CrossStitchAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.in_dim = 1024\n        self.num_classes = Config.num_classes\n        self.cross_stitch = CrossStitchUnit()\n        \n        self.fc_cls1 = LinBNReLU(self.in_dim, 512)\n        self.fc_reg1 = LinBNReLU(self.in_dim, 512)\n        self.fc_vol1 = LinBNReLU(self.in_dim, 512)\n    \n        self.cross_stitch2 = CrossStitchUnit()\n        \n        self.fc_cls2 = nn.Linear(512, self.num_classes)\n        self.fc_reg2 = nn.Linear(512, 1) # Predict Weight and Class of the item.\n        self.fc_vol2 = nn.Linear(512, 1)\n    def forward(self, classification, regression, volume):\n        classification, regression, volume = self.cross_stitch(classification, regression, volume)\n        \n        if len(classification.shape) < 2:\n            classification = classification.unsqueeze(0)\n        if len(regression.shape) < 2:\n            regression = regression.unsqueeze(0)\n        if len(volume.shape) < 2:\n            volume = volume.unsqueeze(0)\n            \n        fc_cls1 = self.fc_cls1(classification)\n        fc_reg1 = self.fc_reg1(regression)\n        fc_vol1 = self.fc_vol1(volume)\n        \n        fc_cls1, fc_reg1, fc_vol1 = self.cross_stitch2(fc_cls1, fc_reg1, fc_vol1)\n        \n        fc_cls2 = self.fc_cls2(fc_cls1)\n        fc_reg2 = torch.squeeze(self.fc_reg2(fc_reg1))\n        fc_vol2 = torch.squeeze(self.fc_vol2(fc_vol1))\n        return fc_cls2, fc_reg2, fc_vol2\nclass ModelAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.featureExtractor = FeaturesAlpha()\n        self.crossStitch = CrossStitchAlpha()#BaseLineHeadAlpha()\n    def forward(self, x):\n        features= self.featureExtractor(x)\n        return self.crossStitch(*features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Code","metadata":{}},{"cell_type":"code","source":"class Accuracy():\n    def __init__(self):\n        self.accuracy = 0\n        self.count = 0\n    def update_state(self, y_pred, y_true):\n        y_pred = F.softmax(y_pred, dim = -1)\n        _, y_pred = torch.max(y_pred, dim = -1) \n        B = y_pred.shape[0]\n        acc = (torch.sum(y_pred == y_true) / B).item()\n        self.accuracy += acc\n        self.count += 1\n    def reset_states(self):\n        self.accuracy = 0\n        self.count = 0\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.accuracy / self.count, 3)\n        return 0\nclass F1Score():\n    # Metric For CLS\n    def __init__(self):\n        self.f1_score = 0\n        self.count = 0 \n    def update_state(self, y_pred, y_true):\n        y_pred = F.softmax(y_pred, dim = -1)\n        _, y_pred = torch.max(y_pred, dim = -1)\n        f1 = metrics.f1_score(y_true.cpu(), y_pred.cpu(), average = 'micro')\n        self.f1_score += f1\n        self.count += 1\n    def reset_states(self):\n        self.count = 0\n        self.f1_score = 0\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.f1_score / self.count, 3)\n        return  \nclass L2Distance():\n    def __init__(self):\n        self.L2Loss = nn.MSELoss()\n        self.dist = 0.0\n        self.count = 0\n    def update_state(self, y_pred_reg, y_true_reg, y_pred_vol, y_true_vol):\n        self.dist += self.L2Loss(y_pred_reg, y_true_reg.float()).item()\n        self.dist += self.L2Loss(y_pred_vol, y_true_vol.float()).item()\n        self.count += 1\n    def reset_states(self):\n        self.dist = 0\n        self.count = 0\n    @property \n    def value(self):\n        if self.count != 0:\n            return round(self.dist / self.count, 3)\n        return 0.0\nclass LossAlpha():\n    def __init__(self):\n        self.L2Loss = nn.MSELoss()\n        self.CELoss = nn.CrossEntropyLoss()\n        \n        self.loss = 0\n        self.count = 0 \n    def update_state(self, y_pred_cls, y_true_cls, y_pred_reg, y_true_reg, y_pred_vol, y_true_vol):\n        regloss = self.L2Loss(y_pred_reg, y_true_reg.float()) / 5000\n        volloss = self.L2Loss(y_pred_vol, y_true_vol.float()) / 5000\n        celoss = self.CELoss(y_pred_cls, y_true_cls)  \n        \n        loss = regloss + volloss + celoss\n        self.loss += loss.item()\n        self.count += 1\n        return loss\n    def reset_states(self):\n        self.loss = 0\n        self.count = 0\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.loss / self.count, 3)\n        return 999999","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlphaTrainer(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n        \n        self.TrainLoss = LossAlpha()\n        self.ValLoss = LossAlpha()\n        self.ValF1 = F1Score()\n        self.ValL2 = L2Distance()\n        self.ValAccuracy = Accuracy()\n        self.best = {'loss': float('inf'), 'f1': 0.0, 'l2': float('inf')}\n        \n        self.EPOCHS = 0\n    def configure_model(self):\n        model = ModelAlpha()\n        return model\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr = TrainConfig.lr, weight_decay = TrainConfig.weight_decay)\n        #self.lr_decay = optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, 1e-8)\n        #self.lr_decay_cycle = optim.lr_scheduler.OneCycleLR(optimizer, TrainConfig.max_lr, total_steps = TrainConfig.num_steps_per_epoch * TrainConfig.num_epochs, epochs = TrainConfig.num_epochs, steps_per_epoch = TrainConfig.num_steps_per_epoch)\n        return optimizer\n    def training_step(self, batch, batch_idx):\n        x, class_idx, volume, weight = batch\n        pred_class, pred_weight, pred_volume = self.model(x)\n        \n        loss = self.TrainLoss.update_state(pred_class, class_idx, pred_weight, weight, pred_volume, volume)\n        #self.lr_decay_cycle.step()\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, class_idx, volume, weight = batch\n        pred_class, pred_weight, pred_volume = self.model(x)\n        self.ValAccuracy.update_state(pred_class, class_idx)\n        self.ValLoss.update_state(pred_class, class_idx, pred_weight, weight, pred_volume, volume)\n        self.ValL2.update_state(pred_weight, weight, pred_volume, volume)\n        self.ValF1.update_state(pred_class, class_idx)\n    def reset_states(self):\n        self.EPOCHS += 1\n        self.TrainLoss.reset_states()\n        self.ValLoss.reset_states()\n        self.ValL2.reset_states()\n        self.ValF1.reset_states()\n    def print_states(self):\n        trainLoss = self.TrainLoss.value\n        valLoss = self.ValLoss.value\n        valF1 = self.ValF1.value\n        valL2 = self.ValL2.value\n        valAccuracy = self.ValAccuracy.value\n        self.log('val_loss', valLoss)\n        #self.lr_decay.step()\n        if valLoss < self.best['loss']:\n            self.best['loss'] = valLoss\n            torch.save(self.state_dict(), \"./loss.pth\")\n        if valF1 > self.best['f1']:\n            self.best['f1'] = valF1\n            torch.save(self.state_dict(), './f1.pth')\n        if valL2 < self.best['l2']:\n            self.best['l2'] = valL2\n            torch.save(self.state_dict(), './l2.pth')\n        print(f\"E: {self.EPOCHS},VA: {valAccuracy} TL: {trainLoss}, VL: {valLoss}, F1: {valF1}, L2: {valL2}, BL: {self.best['loss']}, BL2: {self.best['l2']}, BF1: {self.best['f1']}\")\n        \n        \n    def validation_epoch_end(self, logs):\n        self.print_states()\n        self.reset_states()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainAlpha():\n    model = AlphaTrainer()\n    train = DataModule.get_cls()\n    val = DataModule.get_cls(train = False)\n    cbs = [pl.callbacks.EarlyStopping(verbose = True,monitor = 'val_loss', patience = 20)]\n    dls = DataLoaders.from_dsets(train, val, batch_size = TrainConfig.batch_size, num_workers = TrainConfig.num_workers, shuffle = True, pin_memory = True)\n    if torch.cuda.is_available(): model.cuda(), dls.cuda()\n    \n    trainer = pl.Trainer(logger = None, callbacks = cbs, checkpoint_callback= False, check_val_every_n_epoch=1, gpus=1, max_epochs=TrainConfig.num_epochs, num_sanity_val_steps=0)\n    trainer.fit(model, dls[0], dls[1])","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainAlpha()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Code: Testing on a Few Samples","metadata":{}},{"cell_type":"code","source":"class WeightWatcherAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.weights2Cal ={\n            'apple': 0.52,\n            'banana': 0.89,\n            'bread': 3.15,\n            'bun':2.23,\n            'doughnut':4.34,\n            'egg': 1.43,\n            'fired_dough_twist': 24.16,\n            'grape': 0.69,\n            'lemon': 0.29,\n            'litchi': 0.66,\n            'mango': 0.60,\n            'mooncake': 18.83,\n            'orange': 0.63,\n            'peach': 0.57,\n            'pear': 0.39,\n            'plum': 0.46,\n            'qiwi': 0.61,\n            'sachima': 21.45,\n            'tomato': 0.27\n        }\n        \n        self.cls2idx = {list(self.weights2Cal.keys())[idx]: idx for idx in range(len(self.weights2Cal))}\n        self.idx2cls = {idx: list(self.weights2Cal.keys())[idx] for idx in range(len(self.weights2Cal))}\n        \n        self.weight_path = '../input/weightwatcher/f1.pth'\n        self.model = ModelAlpha()\n        self.load_state_dict(torch.load(self.weight_path, map_location = self.device))\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            cls_idx, weights, volume = self.model(x)\n            cls_idx = F.softmax(cls_idx, dim = -1)\n            _, cls_idx = torch.max(cls_idx, dim = -1)\n            cls_idx = cls_idx.item()\n            \n            class_val = self.idx2cls[cls_idx]\n            weight = weights.item()\n            calories = self.weights2Cal[class_val] * weights\n            \n            return class_val, weights, volume, calories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = WeightWatcherAlpha()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loader = DataModule.get_cls(train = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for images, class_idx, volume, weight in val_loader:\n    cls_pred, weight_pred, volume_pred, calories_pred = model(images.unsqueeze(0))\n    plt.imshow(images.transpose(0, 1).transpose(1, 2))\n    plt.show()\n    print('-----------')\n    print(class_idx)\n    print(cls_pred)\n    print('-----')\n    print(weight)\n    print(weight_pred)\n    print('-----------')\n    print(volume)\n    print(volume_pred)\n    print('-----------------')\n    print(calories_pred)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]}]}